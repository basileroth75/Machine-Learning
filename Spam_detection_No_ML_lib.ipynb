{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering spam messages with Naive Bayes theorem\n",
    "\n",
    "#### The goal of this project was to train a model based on the Bayes probabity theorem to detect if an email is a SPAM or not.\n",
    "- Preparing the data\n",
    "- Extracing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load the messages and the labels from the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yup i've finished c ü there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Remember to ask alex about his pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>No da..today also i forgot..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ola would get back to you maybe not today but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fwiw the reason I'm only around when it's time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hello, my boytoy! I made it home and my consta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Congrats kano..whr s the treat maga?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>Who u talking about?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Status                                            Message\n",
       "0    ham                     Yup i've finished c ü there...\n",
       "1    ham               Remember to ask alex about his pizza\n",
       "2    ham                       No da..today also i forgot..\n",
       "3    ham  Ola would get back to you maybe not today but ...\n",
       "4    ham  Fwiw the reason I'm only around when it's time...\n",
       "5    ham  Hello, my boytoy! I made it home and my consta...\n",
       "6    ham               Congrats kano..whr s the treat maga?\n",
       "7    ham                               Who u talking about?\n",
       "8    ham                                             Yup...\n",
       "9    ham                                              Ok..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Function which returns a dataframe from our text file\n",
    "def create_dataframe_from_file(filename):\n",
    "    df = pd.DataFrame(columns=['Status','Message'])\n",
    "    with open('messages.txt') as file:\n",
    "        for line in file:\n",
    "            Id,rest_of_string = line.split(\"\\t\")\n",
    "            rest_of_string,go_back_line = rest_of_string.split(\"\\n\")\n",
    "            df = df.append({'Status' : Id , 'Message' : rest_of_string} , ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def create_dataframe_from_file(filename):\n",
    "    df = pd.read_csv(filename, sep = '\\t',names=['Status','Message'],encoding='utf-8',quoting=csv.QUOTE_NONE);\n",
    "    return df\n",
    "\n",
    "messages_df = create_dataframe_from_file(\"messages.txt\")\n",
    "messages_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Clean the dataset and get info on it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of messages : 5000\n",
      "# of spams : 672\n",
      "# of nnon spams : 4328\n",
      "   Status                                            Message\n",
      "0       0                     Yup i've finished c ü there...\n",
      "1       0               Remember to ask alex about his pizza\n",
      "2       0                       No da..today also i forgot..\n",
      "3       0  Ola would get back to you maybe not today but ...\n",
      "4       0  Fwiw the reason I'm only around when it's time...\n"
     ]
    }
   ],
   "source": [
    "# We replace our \"Ham\" and \"Spam\" labels by 0s and 1s\n",
    "cleaned_messages_df = messages_df.replace({'Status': {'ham': 0, 'spam': 1}})\n",
    "\n",
    "# We get the number of text messages and the spam/ham count in our df\n",
    "df_size = cleaned_messages_df.shape[0]\n",
    "num_spams = (cleaned_messages_df.Status == 1).sum()\n",
    "num_hams = (cleaned_messages_df.Status == 0).sum()\n",
    "print(\"Total # of messages :\",df_size)\n",
    "print(\"# of spams :\",num_spams)\n",
    "print(\"# of nnon spams :\",num_hams)\n",
    "print(cleaned_messages_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Split the data in training and testing examples\n",
    "- The Spam values are replaced by 1 and Ham values by 0\n",
    "- Then we choose to arbitraly to split our data on a 70-30 basis for our training purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3500 training examples.\n",
      "There are 1500 testing examples.\n"
     ]
    }
   ],
   "source": [
    "# We choose to use 70% of this for training purpose\n",
    "train_size = round(df_size*0.70)\n",
    "test_size = df_size - train_size\n",
    "\n",
    "# We create the train and test datasets\n",
    "df_train = cleaned_messages_df[:train_size]\n",
    "df_test = cleaned_messages_df[train_size:]\n",
    "\n",
    "print(\"There are {} training examples.\".format(train_size))\n",
    "print(\"There are {} testing examples.\".format(test_size))\n",
    "# df_labels_ToList = df_test_label['Status'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Create a dictionnary  a dictionnary from the words in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words :  3000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create dictionnary\n",
    "def make_Dictionary(dataset):\n",
    "    # List of all the words\n",
    "    all_words = []\n",
    "    # Loop through the whole dataset\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Get each string\n",
    "        full_line = row['Message']\n",
    "        # Split it to obtain all the words \n",
    "        words = full_line.split()\n",
    "        # Add them to our tab containing all the words\n",
    "        all_words += words\n",
    "    # We use the counter function to create a tab with all the different words and the number of times they occur\n",
    "    dictionary = Counter(all_words)\n",
    "    \n",
    "    for item in list(dictionary): \n",
    "        if item.isalpha() == False:\n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    # Get the 3000 first most common words\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary\n",
    "\n",
    "training_dict = make_Dictionary(df_train)\n",
    "print(\"# of words : \", len(training_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Extract features from both the training data and test data.\n",
    "\n",
    "For each message, it means checking for every word if it appears in the dictionnary. Each message is going to be a 3000 array long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature dataframe for training is (3500, 3000)\n",
      "The shape of the feature dataframe for testing is (1500, 3000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_features(dataset,dictionary):\n",
    "    features_matrix = np.zeros((len(dataset), 3000))\n",
    "    docID = 0\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Get each string\n",
    "        full_line = row['Message']\n",
    "        # Split it to obtain all the words \n",
    "        words = full_line.split()\n",
    "        # Go through all the words in the message\n",
    "        for word in words:\n",
    "            wordID = 0\n",
    "            # We are going to check if the word is in the dictionnary or not\n",
    "            for i, d in enumerate(dictionary):\n",
    "                # If it is, set the flag to 1\n",
    "                if d[0] == word:\n",
    "                    wordID = i\n",
    "                    features_matrix[docID, wordID] = words.count(word)\n",
    "        # Now work on the next feature\n",
    "        docID = docID + 1\n",
    "    return features_matrix\n",
    "\n",
    "extract_feats_train = extract_features(df_train,training_dict)\n",
    "extract_feats_test = extract_features(df_test,training_dict)\n",
    "print(\"The shape of the feature dataframe for training is\",extract_feats_train.shape)\n",
    "print(\"The shape of the feature dataframe for testing is\",extract_feats_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Use the Naive Bayes theorem to build and fit a model from the training data\n",
    "\n",
    "We are trying to evaluate the probability of an email being a Spam or Ham depending on the words in the text :\n",
    "![title](formulas.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Index            Word  N_S  N_H  N_H/N_tot_H  N_S/N_tot_S  Total_ham  \\\n",
      "0         0              to  271  742     0.244730     0.581197   0.212146   \n",
      "1         1             you  103  686     0.226285     0.222222   0.196156   \n",
      "2         2             the   96  513     0.169302     0.207265   0.146760   \n",
      "3         3             and   60  361     0.119236     0.130342   0.103360   \n",
      "4         4              in   33  410     0.135375     0.072650   0.117351   \n",
      "...     ...             ...  ...  ...          ...          ...        ...   \n",
      "2995   2995             Lil    0    1     0.000659     0.002137   0.000571   \n",
      "2996   2996         Thinkin    0    1     0.000659     0.002137   0.000571   \n",
      "2997   2997         showers    0    1     0.000659     0.002137   0.000571   \n",
      "2998   2998  possessiveness    0    1     0.000659     0.002137   0.000571   \n",
      "2999   2999          poured    0    1     0.000659     0.002137   0.000571   \n",
      "\n",
      "      Total_spam  \n",
      "0       0.077382  \n",
      "1       0.029587  \n",
      "2       0.027596  \n",
      "3       0.017354  \n",
      "4       0.009673  \n",
      "...          ...  \n",
      "2995    0.000284  \n",
      "2996    0.000284  \n",
      "2997    0.000284  \n",
      "2998    0.000284  \n",
      "2999    0.000284  \n",
      "\n",
      "[3000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "\n",
    "def build_model(train_features,targets,dictionnary):\n",
    "    df_model = pd.DataFrame(columns=['Index','Word','N_S','N_H','N_H/N_tot_H','N_S/N_tot_S','Total_ham','Total_spam'])\n",
    "    df_model['Word'] = [row[0] for row in dictionnary]\n",
    "    df_model['N_S'] = 0\n",
    "    df_model['N_H'] = 0\n",
    "    df_model['N_H/N_tot_H'] = 0\n",
    "    df_model['N_S/N_tot_S'] = 0\n",
    "    df_model['Total_ham'] = 0\n",
    "    df_model['Total_spam'] = 0\n",
    "    # Fill the indexes with the 3000 indexes of the dictionnary\n",
    "    for i in range(0,len(dictionnary),1):\n",
    "        df_model.iloc[i,0] = i    \n",
    "    # Go through the features and see for each word if they correspond to a spam or ham message    \n",
    "    for vector_index, vector in enumerate(train_features): # Go through 3500 vectors\n",
    "        # Now go through the feature vector to check if this word is in the message\n",
    "        for word_index, zero_or_one in enumerate(vector): # Go through the 3000 ones and zeros\n",
    "            # If it is a spam and the word is in the message\n",
    "            if zero_or_one >= 1 and targets[vector_index] >= 1:\n",
    "                df_model.iloc[word_index,2] = df_model.iloc[word_index,2] + 1\n",
    "            # If it is a ham and the word is in the message\n",
    "            if zero_or_one >= 1 and targets[vector_index] == 0:\n",
    "                df_model.iloc[word_index,3] = df_model.iloc[word_index,3] + 1\n",
    "                       \n",
    "    total_spam = np.count_nonzero(targets)\n",
    "    total_ham = len(targets) - total_spam\n",
    "\n",
    "    log_prob_spam = np.log(total_spam/(total_ham+total_spam))\n",
    "    log_prob_ham = np.log(total_ham/(total_ham+total_spam))\n",
    "    proba_spam = total_spam/(total_ham+total_spam)\n",
    "    proba_ham = total_ham/(total_ham+total_spam)\n",
    "    #print(proba_spam)\n",
    "    #print(proba_ham)\n",
    "    \n",
    "    for i in range(0,len(dictionnary),1):\n",
    "        # The columns will contain respectively P(word|spam) and P(word|ham)\n",
    "        df_model.iloc[i,4] = (df_model.iloc[i,3]+1) / (total_ham+2)\n",
    "        df_model.iloc[i,5] = (df_model.iloc[i,2]+1) / (total_spam+2)\n",
    "        # The columns will contain respectively P(word|spam)*P(spam) and P(word|ham)*P(ham)\n",
    "        df_model.iloc[i,6] = (df_model.iloc[i,4])*(proba_ham)\n",
    "        df_model.iloc[i,7] = (df_model.iloc[i,5])*(proba_spam)\n",
    "    return df_model\n",
    "                            \n",
    "model = build_model(extract_feats_train,df_train['Status'],training_dict)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Use the model to make predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy =  97.13333333333334\n"
     ]
    }
   ],
   "source": [
    "def predict_log_proba(extract_feats,model):\n",
    "    feature_prob_5 = model.loc[ : ,'Total_ham'].values\n",
    "    feature_prob_6 = model.loc[ : ,'Total_spam'].values\n",
    "    feature_prob_5 = feature_prob_5.reshape(1, 3000)\n",
    "    feature_prob_6 = feature_prob_6.reshape(1, 3000)\n",
    "    \n",
    "    feature_prob_1 = model.loc[ : ,'N_H/N_tot_H'].values\n",
    "    feature_prob_2 = model.loc[ : ,'N_S/N_tot_S'].values\n",
    "    feature_prob_1 = feature_prob_1.reshape(1, 3000)\n",
    "    feature_prob_2 = feature_prob_2.reshape(1, 3000)\n",
    "    \n",
    "    matrix = []\n",
    "    tot = np.concatenate((feature_prob_1, feature_prob_2), axis=0)\n",
    "    log_prob_per_classs = np.array(model.iloc[0,6], model.iloc[0,7])\n",
    "    \n",
    "    for x in extract_feats:\n",
    "        # predict_vector = (np.log(tot) * x + log_prob_per_class ).sum(axis=1)    \n",
    "        predict_vector = (np.log(tot) * x + np.log(1 - tot) * np.abs(x - 1)).sum(axis=1) + log_prob_per_classs\n",
    "        matrix.append(predict_vector)\n",
    "    return matrix\n",
    "\n",
    "def predict(result):\n",
    "    return np.argmax(result, axis=1)\n",
    "\n",
    "def get_accuracy(predictions,labels):\n",
    "    total = 0\n",
    "    for i in range(0,len(predictions),1):\n",
    "        if predictions[i] == labels[i]:\n",
    "            total = total + 1\n",
    "    return (total/len(predictions))*100\n",
    "\n",
    "result = predict_log_proba(extract_feats_test, model)\n",
    "final_predictions = predict(result)\n",
    "df_labels_ToList = df_test['Status'].tolist()\n",
    "print(\"Total accuracy = \", get_accuracy(final_predictions,df_labels_ToList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Compute metrics to evaluate the performance of the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 97.13333333333334\n",
      "Precision : 93.58288770053476\n",
      "Recall : 84.95145631067962\n",
      "F1-Score : 0.89058524173028\n"
     ]
    }
   ],
   "source": [
    "cpt_true_negative = 0\n",
    "cpt_false_negative = 0\n",
    "cpt_true_positive = 0\n",
    "cpt_false_positive = 0\n",
    "\n",
    "for i in range (0,len(final_predictions),1):\n",
    "    if final_predictions[i] == df_labels_ToList[i] and final_predictions[i] == 0:\n",
    "        cpt_true_negative = cpt_true_negative + 1\n",
    "    if final_predictions[i] != df_labels_ToList[i] and final_predictions[i] == 0:\n",
    "        cpt_false_negative = cpt_false_negative +1\n",
    "    if final_predictions[i] == df_labels_ToList[i] and final_predictions[i] == 1:\n",
    "        cpt_true_positive = cpt_true_positive + 1\n",
    "    if final_predictions[i] != df_labels_ToList[i] and final_predictions[i] == 1:\n",
    "        cpt_false_positive = cpt_false_positive + 1\n",
    "\n",
    "def compute_accuracy(tp, tn, fn, fp):\n",
    "    return ((tp + tn) * 100)/ float( tp + tn + fn + fp)\n",
    "\n",
    "def compute_precision(tp, fp):\n",
    "    return (tp  * 100)/ float( tp + fp)\n",
    "\n",
    "def compute_recall(tp, fn):\n",
    "    return (tp  * 100)/ float( tp + fn)\n",
    "\n",
    "def compute_f1_score(tp, tn, fn, fp):\n",
    "    # calculates the F1 score\n",
    "    precision = compute_precision(tp, fp)/100\n",
    "    recall = compute_recall(tp, fn)/100\n",
    "    f1_score = (2*precision*recall)/ (precision + recall)\n",
    "    return f1_score\n",
    "        \n",
    "accuracy = compute_accuracy(cpt_true_positive, cpt_true_negative, cpt_false_negative, cpt_false_positive)\n",
    "precision = compute_precision(cpt_true_positive, cpt_false_positive)\n",
    "recall = compute_recall(cpt_true_positive, cpt_false_negative)\n",
    "f1 = compute_f1_score(cpt_true_positive, cpt_true_negative, cpt_false_negative, cpt_false_positive)\n",
    "    \n",
    "print(\"Accuracy :\",accuracy)\n",
    "print(\"Precision :\",precision)\n",
    "print(\"Recall :\",recall)\n",
    "print(\"F1-Score :\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
