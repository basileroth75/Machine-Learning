{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering spam messages with Naive Bayes theorem\n",
    "\n",
    "#### The goal of this project was to train a model based on the Bayes probabity theorem to detect if an email is a SPAM or not.\n",
    "- Preparing the data\n",
    "- Extracing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load the messages and the labels from the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yup i've finished c ü there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Remember to ask alex about his pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>No da..today also i forgot..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ola would get back to you maybe not today but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fwiw the reason I'm only around when it's time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hello, my boytoy! I made it home and my consta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Congrats kano..whr s the treat maga?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>Who u talking about?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Status                                            Message\n",
       "0    ham                     Yup i've finished c ü there...\n",
       "1    ham               Remember to ask alex about his pizza\n",
       "2    ham                       No da..today also i forgot..\n",
       "3    ham  Ola would get back to you maybe not today but ...\n",
       "4    ham  Fwiw the reason I'm only around when it's time...\n",
       "5    ham  Hello, my boytoy! I made it home and my consta...\n",
       "6    ham               Congrats kano..whr s the treat maga?\n",
       "7    ham                               Who u talking about?\n",
       "8    ham                                             Yup...\n",
       "9    ham                                              Ok..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Function which returns a dataframe from our text file\n",
    "def create_dataframe_from_file(filename):\n",
    "    df = pd.DataFrame(columns=['Status','Message'])\n",
    "    with open('messages.txt') as file:\n",
    "        for line in file:\n",
    "            Id,rest_of_string = line.split(\"\\t\")\n",
    "            rest_of_string,go_back_line = rest_of_string.split(\"\\n\")\n",
    "            df = df.append({'Status' : Id , 'Message' : rest_of_string} , ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def create_dataframe_from_file(filename):\n",
    "    df = pd.read_csv(filename, sep = '\\t',names=['Status','Message'],encoding='utf-8',quoting=csv.QUOTE_NONE);\n",
    "    return df\n",
    "\n",
    "messages_df = create_dataframe_from_file(\"messages.txt\")\n",
    "messages_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Clean the dataset and get info on it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of messages : 5000\n",
      "# of spams : 672\n",
      "# of nnon spams : 4328\n",
      "   Status                                            Message\n",
      "0       0                     Yup i've finished c ü there...\n",
      "1       0               Remember to ask alex about his pizza\n",
      "2       0                       No da..today also i forgot..\n",
      "3       0  Ola would get back to you maybe not today but ...\n",
      "4       0  Fwiw the reason I'm only around when it's time...\n"
     ]
    }
   ],
   "source": [
    "# We replace our \"Ham\" and \"Spam\" labels by 0s and 1s\n",
    "cleaned_messages_df = messages_df.replace({'Status': {'ham': 0, 'spam': 1}})\n",
    "\n",
    "# We get the number of text messages and the spam/ham count in our df\n",
    "df_size = cleaned_messages_df.shape[0]\n",
    "num_spams = (cleaned_messages_df.Status == 1).sum()\n",
    "num_hams = (cleaned_messages_df.Status == 0).sum()\n",
    "print(\"Total # of messages :\",df_size)\n",
    "print(\"# of spams :\",num_spams)\n",
    "print(\"# of nnon spams :\",num_hams)\n",
    "print(cleaned_messages_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Split the data in training and testing examples\n",
    "- The Spam values are replaced by 1 and Ham values by 0\n",
    "- Then we choose to arbitraly to split our data on a 70-30 basis for our training purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3500 training examples.\n",
      "There are 1500 testing examples.\n"
     ]
    }
   ],
   "source": [
    "# We choose to use 70% of this for training purpose\n",
    "train_size = round(df_size*0.70)\n",
    "test_size = df_size - train_size\n",
    "\n",
    "# We create the train and test datasets\n",
    "df_train = cleaned_messages_df[:train_size]\n",
    "df_test = cleaned_messages_df[train_size:]\n",
    "\n",
    "print(\"There are {} training examples.\".format(train_size))\n",
    "print(\"There are {} testing examples.\".format(test_size))\n",
    "# df_labels_ToList = df_test_label['Status'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Create a dictionnary  a dictionnary from the words in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words :  3000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create dictionnary\n",
    "def make_Dictionary(dataset):\n",
    "    # List of all the words\n",
    "    all_words = []\n",
    "    # Loop through the whole dataset\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Get each string\n",
    "        full_line = row['Message']\n",
    "        # Split it to obtain all the words \n",
    "        words = full_line.split()\n",
    "        # Add them to our tab containing all the words\n",
    "        all_words += words\n",
    "    # We use the counter function to create a tab with all the different words and the number of times they occur\n",
    "    dictionary = Counter(all_words)\n",
    "    \n",
    "    for item in list(dictionary): \n",
    "        if item.isalpha() == False:\n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    # Get the 3000 first most common words\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary\n",
    "\n",
    "training_dict = make_Dictionary(df_train)\n",
    "print(\"# of words : \", len(training_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Extract features from both the training data and test data.\n",
    "\n",
    "For each message, it means checking for every word if it appears in the dictionnary. Each message is going to be a 3000 array long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature dataframe for training is (3500, 3000)\n",
      "The shape of the feature dataframe for testing is (1500, 3000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_features(dataset,dictionary):\n",
    "    features_matrix = np.zeros((len(dataset), 3000))\n",
    "    docID = 0\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Get each string\n",
    "        full_line = row['Message']\n",
    "        # Split it to obtain all the words \n",
    "        words = full_line.split()\n",
    "        # Go through all the words in the message\n",
    "        for word in words:\n",
    "            wordID = 0\n",
    "            # We are going to check if the word is in the dictionnary or not\n",
    "            for i, d in enumerate(dictionary):\n",
    "                # If it is, set the flag to 1\n",
    "                if d[0] == word:\n",
    "                    wordID = i\n",
    "                    features_matrix[docID, wordID] = words.count(word)\n",
    "        # Now work on the next feature\n",
    "        docID = docID + 1\n",
    "    return features_matrix\n",
    "\n",
    "extract_feats_train = extract_features(df_train,training_dict)\n",
    "extract_feats_test = extract_features(df_test,training_dict)\n",
    "print(\"The shape of the feature dataframe for training is\",extract_feats_train.shape)\n",
    "print(\"The shape of the feature dataframe for testing is\",extract_feats_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Use the model to make predictions for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Measure the spam-filtering performance through the confusion matrix from the Sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1293    1]\n",
      " [  39  167]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1294\n",
      "           1       0.99      0.81      0.89       206\n",
      "\n",
      "    accuracy                           0.97      1500\n",
      "   macro avg       0.98      0.90      0.94      1500\n",
      "weighted avg       0.97      0.97      0.97      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fait avec sklearn pour comparer\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Training Naive bayes classifier and its variants\n",
    "# model2 = MultinomialNB()\n",
    "model2 = BernoulliNB()\n",
    "model2.fit(extract_feats_train, df_train['Status'])\n",
    "# Test the unseen mails for Spam\n",
    "\n",
    "results2 = model2.predict(extract_feats_test)\n",
    "print(confusion_matrix(df_test['Status'], results2))\n",
    "print (metrics.classification_report(df_test['Status'], results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
